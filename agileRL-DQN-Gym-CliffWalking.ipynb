{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"72c1d9921c6c44d581c88448bbb1e1b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25dfa03222724f99a0e650d24ae855b9","IPY_MODEL_7cb6e7855d9d452eb6df117277db4404","IPY_MODEL_f71e1945a7574b678a7fff59742eb757"],"layout":"IPY_MODEL_0b9ccd2c4430405ea3c0a127ee68a041"}},"25dfa03222724f99a0e650d24ae855b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c92470fbbe904f679478847e60a04cc3","placeholder":"​","style":"IPY_MODEL_2b3e387dfd5a47b0b4abbc5530a88188","value":"100%"}},"7cb6e7855d9d452eb6df117277db4404":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71ad7183356c4300b3f9cca60bda639e","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9d18fa98f2042118e925d30a552583f","value":100000}},"f71e1945a7574b678a7fff59742eb757":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c3ba271f54649108637883526bbf023","placeholder":"​","style":"IPY_MODEL_c4115c592a7c4457b9d7479a65062fd1","value":" 100000/100000 [07:46&lt;00:00, 214.25step/s]"}},"0b9ccd2c4430405ea3c0a127ee68a041":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c92470fbbe904f679478847e60a04cc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b3e387dfd5a47b0b4abbc5530a88188":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71ad7183356c4300b3f9cca60bda639e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9d18fa98f2042118e925d30a552583f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c3ba271f54649108637883526bbf023":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4115c592a7c4457b9d7479a65062fd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Training a Deep Q Learning Neural Network Agent in Gymnasium's CliffWalking-v0 Discrete Environment using AgileRL\n"],"metadata":{"id":"K19cABMcwRRV"}},{"cell_type":"markdown","source":["## Imports and Installs"],"metadata":{"id":"hdija3UgzqNa"}},{"cell_type":"code","source":["!pip install gym gym[atari] gym[accept-rom-license] agilerl accelerate>=0.21.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EN99d6fxuALj","executionInfo":{"status":"ok","timestamp":1731191018875,"user_tz":480,"elapsed":3596,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"}},"outputId":"f81640ee-4781-490d-ddaf-443267dc2182"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","import gymnasium as gym\n","import numpy as np\n","\n","### These imports will be used to implement the NN Agent ##\n","import torch\n","from agilerl.algorithms.dqn import DQN\n","from agilerl.components.replay_buffer import ReplayBuffer\n","from agilerl.training.train_off_policy import train_off_policy\n","from agilerl.utils.utils import create_population, make_vect_envs\n","# import trange\n","from tqdm.notebook import trange\n","\n","from tqdm import tqdm\n","from __future__ import annotations\n","from collections import defaultdict"],"metadata":{"id":"g-Pz7XjLsdab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setting up the Reinforcement Learning Environment in Gym\n","\n","In this example notebook, we chose to use the 'CliffWalking-v0' environment prior to tackling the Blackjack environment as the CliffWalking environment has discrete observations and actions, thus it is easier to use with AgileRL's DQN network class. Similarly, gym.vector.make was also needed in order to convert the observation and action data types to vectors or arrays - required for use with the DQN. Thus, this was a learning experience that we used to build our own custom DQN and training and evaluation in a seperate notebook for use with the Blackjack environment. Another setback faced when using the AgileRL library is that it had difficulties training with a GPU device (in colab notebooks). Since we are not dealing with a computationally heavy environment GPU support is not required."],"metadata":{"id":"j4YGBETEszni"}},{"cell_type":"code","source":["#env = make_vect_envs(\"CliffWalking-v0\", num_envs=1) # uncomment if want to run across envs\n","env = gym.vector.make(\"CliffWalking-v0\")"],"metadata":{"id":"bfrlHR2ptyTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    state_dim = env.single_observation_space.n  # Discrete observation space\n","    one_hot = True  # Requires one-hot encoding\n","    is_discrete_obs = True\n","except Exception:\n","    state_dim = env.single_observation_space.shape  # Continuous observation space\n","    one_hot = False  # Does not require one-hot encoding\n","    is_discrete_obs = False\n","try:\n","    action_dim = env.single_action_space.n  # Discrete action space\n","    is_discrete_actions = True\n","except Exception:\n","    action_dim = env.single_action_space.shape[0]  # Continuous action space\n","    is_discrete_actions = False"],"metadata":{"id":"Kv0pReTw9E0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Action dimension: {action_dim}\")\n","print(f\"Observation dimension: {state_dim}\")\n","print(f\"Is discrete action space: {is_discrete_actions}\")\n","print(f\"Is discrete observation space: {is_discrete_obs}\")\n","print(f\"Is one-hot:  {one_hot}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"spBzzW2B9NA1","executionInfo":{"status":"ok","timestamp":1731191018876,"user_tz":480,"elapsed":9,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"}},"outputId":"e64f6fe8-e5b2-471f-d97a-6b1bc93379a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Action dimension: 4\n","Observation dimension: 48\n","Is discrete action space: True\n","Is discrete observation space: True\n","Is one-hot:  True\n"]}]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"oKFhGDjl1Zph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The blackjack environment does not implement a .shape method on its' observation_space. Since the DQN agent expects a python tuple instead of a gym.tuple, we need to create our own state_dim."],"metadata":{"id":"4XNgnCh22Lrn"}},{"cell_type":"markdown","source":["## Setting up the Deep Q-Learning Agent"],"metadata":{"id":"6v_HZPW-s6Hs"}},{"cell_type":"markdown","source":["In this block we set the default hyperparameters"],"metadata":{"id":"EvFa6q8VyEnU"}},{"cell_type":"code","source":["INIT_HP = {\n","    \"BATCH_SIZE\": 32,  # Smaller batch size\n","    \"LR\": 0.01,  # Much higher learning rate\n","    \"GAMMA\": 0.90,  # Lower discount factor for shorter-term rewards\n","    \"MEMORY_SIZE\": 10_000,\n","    \"LEARN_STEP\": 1,\n","    \"N_STEP\": 1,\n","    \"PER\": False,  # Disable prioritized replay for simplicity\n","    \"TAU\": 0.1,  # Faster target updates\n","    \"NOISY\": False,\n","    \"CHANNELS_LAST\": False,\n","    \"LEARNING_DELAY\": 500,\n","    \"MAX_STEPS\": 100000,  # More steps\n","    \"EVO_STEPS\": 100000,\n","    \"EVAL_STEPS\": 1000,\n","    \"EVAL_LOOP\": 1,\n","}"],"metadata":{"id":"vdhzyK7k2JEe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this block we set the neural network config. Since we are using the more simple discrete observation and action environment, a typical multi-layer perceptron network is sufficient."],"metadata":{"id":"Wqw7r5UcyKL_"}},{"cell_type":"code","source":["# Simpler network\n","NET_CONFIG = {\n","      'arch': 'mlp',\n","      'hidden_size': [64],  # Single hidden layer\n","}"],"metadata":{"id":"9oYejbDLyQrQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, the neural network config is passed to the DQN agent to alter the default network."],"metadata":{"id":"aDYVi0zLyge-"}},{"cell_type":"code","source":["# Initialize agent and memory\n","agent = DQN(\n","    net_config=NET_CONFIG,\n","    batch_size=int(state_dim),\n","    state_dim=[state_dim],\n","    action_dim=action_dim,\n","    one_hot=one_hot,\n","    lr=INIT_HP[\"LR\"],\n","    learn_step=INIT_HP[\"LEARN_STEP\"],\n","    gamma=INIT_HP[\"GAMMA\"],\n","    tau=INIT_HP[\"TAU\"],\n","    device=device)"],"metadata":{"id":"qdEFmBNbwboI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training the DQN Agent in the Blackjack Environment\n","\n","### What is the difference in training with a DQN agent vs a Q-Learning agent?\n","\n","#### Memory and Replay buffer\n","A difference in training an agent with a memory or replay buffer is that the simpler agent.update(state, action, reward, next_state, done) function is decomposed into multiple functions:\n","\n","1. memory.save_to_memory_vect_envs(state, action, reward, next_state, done)\n","2. experience = memory.sample(agent.batch_size)\n","3. agent.learn(experience)\n","\n","This allows for a higher dimensional input used for training (for example with multiple channels or multiple observations).\n","\n","#### Training steps\n","\n","The training steps when using a memory or replay buffer is dependent on the 'batch_size' of the memory. This determines how many 'experiences' / memory samples (or steps in the environment) the memory should be filled with prior to training. Once the memory is filled (this could be taken as the exploration phase as no learning is taking place), the agent continues taking steps while also learning (the training phase)."],"metadata":{"id":"eTFu2xEhwcnN"}},{"cell_type":"code","source":["field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]"],"metadata":{"id":"iry5DfdqtG0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory = ReplayBuffer(memory_size=INIT_HP[\"MEMORY_SIZE\"], field_names=field_names, device=device)"],"metadata":{"id":"MVoJgFRMvTz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# More aggressive exploration\n","eps_start = 1.0\n","eps_end = 0.01\n","eps_decay = 0.995\n"],"metadata":{"id":"mbkAHGS8Yp5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_steps = 0\n","pop = [agent]\n","print(\"Training...\")\n","pbar = trange(INIT_HP[\"MAX_STEPS\"], unit=\"step\")\n","\n","best_fitness = float('-inf')\n","episodes_without_improvement = 0\n","max_episodes_without_improvement = 20\n","\n","while np.less([agent.steps[-1] for agent in pop], INIT_HP[\"MAX_STEPS\"]).all():\n","    pop_episode_scores = []\n","    for agent in pop:\n","        state, info = env.reset()\n","        scores = np.zeros(1)\n","        completed_episode_scores = []\n","        steps = 0\n","        epsilon = max(eps_end, eps_start * (eps_decay ** (total_steps / 1000)))\n","\n","        for idx_step in range(INIT_HP['MAX_STEPS'] // 1):\n","            action = agent.get_action(state, epsilon)\n","            next_state, reward, terminated, truncated, info = env.step(action)\n","\n","            # Modify reward to encourage faster completion\n","            modified_reward = reward - 0.1  # Small penalty for each step\n","\n","            scores += np.array(reward)\n","            steps += 1\n","            total_steps += 1\n","\n","            for idx, (d, t) in enumerate(zip(terminated, truncated)):\n","                if d or t:\n","                    if scores[idx] > -15:  # If performed reasonably well\n","                        modified_reward += 10  # Bonus for good completion\n","                    completed_episode_scores.append(scores[idx])\n","                    agent.scores.append(scores[idx])\n","                    scores[idx] = 0\n","\n","            memory.save_to_memory(\n","                state,\n","                action,\n","                modified_reward,  # Use modified reward\n","                next_state,\n","                terminated,\n","                is_vectorised=True,\n","            )\n","\n","            if len(memory) >= agent.batch_size:\n","                experiences = memory.sample(agent.batch_size)\n","                agent.learn(experiences)\n","\n","            state = next_state\n","\n","            if total_steps % 1000 == 0:\n","                avg_score = np.mean(agent.scores[-100:]) if agent.scores else 0\n","                print(f\"\\nStep {total_steps}: Average Score = {avg_score:.2f}, Epsilon = {epsilon:.3f}\")\n","\n","        pbar.update(INIT_HP['EVO_STEPS'] // len(pop))\n","        agent.steps[-1] += steps\n","        pop_episode_scores.append(completed_episode_scores)\n","\n","        current_fitness = np.mean(completed_episode_scores) if completed_episode_scores else float('-inf')\n","        if current_fitness > best_fitness:\n","            best_fitness = current_fitness\n","            episodes_without_improvement = 0\n","            agent.save_checkpoint('best_cliffwalking.pt')\n","        else:\n","            episodes_without_improvement += 1\n","\n","        if episodes_without_improvement >= max_episodes_without_improvement:\n","            print(\"\\nEarly stopping triggered!\")\n","            break\n","\n","    fitnesses = [\n","        agent.test(\n","            env,\n","            swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n","            max_steps=INIT_HP['EVAL_STEPS'],\n","            loop=INIT_HP['EVAL_LOOP'],\n","        )\n","        for agent in pop\n","    ]\n","    mean_scores = [\n","        (np.mean(episode_scores) if len(episode_scores) > 0 else \"0 completed episodes\")\n","        for episode_scores in pop_episode_scores\n","    ]\n","\n","    print(f\"\\n--- Global steps {total_steps} ---\")\n","    print(f\"Steps {[agent.steps[-1] for agent in pop]}\")\n","    print(f\"Scores: {mean_scores}\")\n","    print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n","    print(f'Best Fitness So Far: {best_fitness:.2f}')\n","\n","    for agent in pop:\n","        agent.steps.append(agent.steps[-1])\n","\n","pbar.close()\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["72c1d9921c6c44d581c88448bbb1e1b4","25dfa03222724f99a0e650d24ae855b9","7cb6e7855d9d452eb6df117277db4404","f71e1945a7574b678a7fff59742eb757","0b9ccd2c4430405ea3c0a127ee68a041","c92470fbbe904f679478847e60a04cc3","2b3e387dfd5a47b0b4abbc5530a88188","71ad7183356c4300b3f9cca60bda639e","a9d18fa98f2042118e925d30a552583f","7c3ba271f54649108637883526bbf023","c4115c592a7c4457b9d7479a65062fd1"]},"id":"Im_4OM54YOFR","executionInfo":{"status":"ok","timestamp":1731191486061,"user_tz":480,"elapsed":466940,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"}},"outputId":"69479e8b-67f5-4907-b7d6-f8cbfea50317"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/100000 [00:00<?, ?step/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c1d9921c6c44d581c88448bbb1e1b4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Step 1000: Average Score = 0.00, Epsilon = 1.000\n","\n","Step 2000: Average Score = 0.00, Epsilon = 1.000\n","\n","Step 3000: Average Score = -21557.00, Epsilon = 1.000\n","\n","Step 4000: Average Score = -21557.00, Epsilon = 1.000\n","\n","Step 5000: Average Score = -21557.00, Epsilon = 1.000\n","\n","Step 6000: Average Score = -21557.00, Epsilon = 1.000\n","\n","Step 7000: Average Score = -21557.00, Epsilon = 1.000\n","\n","Step 8000: Average Score = -21557.00, Epsilon = 1.000\n","\n","Step 9000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 10000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 11000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 12000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 13000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 14000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 15000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 16000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 17000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 18000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 19000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 20000: Average Score = -40291.50, Epsilon = 1.000\n","\n","Step 21000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 22000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 23000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 24000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 25000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 26000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 27000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 28000: Average Score = -52313.50, Epsilon = 1.000\n","\n","Step 29000: Average Score = -48126.50, Epsilon = 1.000\n","\n","Step 30000: Average Score = -48126.50, Epsilon = 1.000\n","\n","Step 31000: Average Score = -48126.50, Epsilon = 1.000\n","\n","Step 32000: Average Score = -48126.50, Epsilon = 1.000\n","\n","Step 33000: Average Score = -48126.50, Epsilon = 1.000\n","\n","Step 34000: Average Score = -48126.50, Epsilon = 1.000\n","\n","Step 35000: Average Score = -50095.57, Epsilon = 1.000\n","\n","Step 36000: Average Score = -50095.57, Epsilon = 1.000\n","\n","Step 37000: Average Score = -50095.57, Epsilon = 1.000\n","\n","Step 38000: Average Score = -50095.57, Epsilon = 1.000\n","\n","Step 39000: Average Score = -50095.57, Epsilon = 1.000\n","\n","Step 40000: Average Score = -50095.57, Epsilon = 1.000\n","\n","Step 41000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 42000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 43000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 44000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 45000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 46000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 47000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 48000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 49000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 50000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 51000: Average Score = -51866.62, Epsilon = 1.000\n","\n","Step 52000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 53000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 54000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 55000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 56000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 57000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 58000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 59000: Average Score = -57890.33, Epsilon = 1.000\n","\n","Step 60000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 61000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 62000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 63000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 64000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 65000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 66000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 67000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 68000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 69000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 70000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 71000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 72000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 73000: Average Score = -60756.30, Epsilon = 1.000\n","\n","Step 74000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 75000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 76000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 77000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 78000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 79000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 80000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 81000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 82000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 83000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 84000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 85000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 86000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 87000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 88000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 89000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 90000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 91000: Average Score = -68658.82, Epsilon = 1.000\n","\n","Step 92000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 93000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 94000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 95000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 96000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 97000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 98000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 99000: Average Score = -78352.67, Epsilon = 1.000\n","\n","Step 100000: Average Score = -78352.67, Epsilon = 1.000\n","\n","--- Global steps 100000 ---\n","Steps [100000]\n","Scores: [-78352.66666666667]\n","Fitnesses: ['-13.00']\n","Best Fitness So Far: -78352.67\n"]}]},{"cell_type":"code","source":["agent.save_checkpoint('CliffWalking.pt')"],"metadata":{"id":"bmm89ErCbjZ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving the Trained Weights and Evaluating the Agent"],"metadata":{"id":"fo1odlCKV6lC"}},{"cell_type":"code","source":["test_env = gym.make(\"CliffWalking-v0\", render_mode='rgb_array')"],"metadata":{"id":"xl9Az9sNhBEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These wrappers are necessary if using the gym.make method instead of the gym.vector.make method to create the test environment. The gym.make method is required if using gym.wrapper.RecordVideo for testing as it sets the render_mode to be able to use rgb_array. The gym.make method forces the environments to use tuples instead of arrays for the data types for observations or actions so is incompatible with the DQN class of AgileRL which expects arrays and not tuples."],"metadata":{"id":"DJzdQ1I_vJzg"}},{"cell_type":"code","source":["class ArrayObservationEnv(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","    # code to overwrite the step and reset functions to modify the state\n","\n","    #super() of env.step\n","    def step(self, action):\n","        obs, reward, terminated, truncated, info = self.env.step(action)\n","        #return tuple([obs]), reward, terminated, truncated, info\n","        return np.array(obs), reward, terminated, truncated, info\n","\n","    #super() of env.reset\n","    def reset(self, **kwargs):\n","        obs, info = self.env.reset(**kwargs)\n","        #return tuple([obs]), info\n","        return np.array(obs), info\n","\n","\n","#wrapper for agileRL DQN\n","\n","class TupletoArrayDQN():\n","    def __init__(self, trained_agent):  # Pass keyword arguments for DQN initialization\n","        self.dqn_instance = trained_agent\n","        #super().__init__() # Added this to solve NameError, not sure if necessary but it works now\n","\n","    #super() of DQN.test\n","    def test(self, env, swap_channels=False, max_steps=None, loop=1):\n","        # uses array env wrapper\n","        env = ArrayObservationEnv(env)\n","        \"\"\"Returns mean test score of agent in environment with epsilon-greedy policy.\n","\n","        :param env: The environment to be tested in\n","        :type env: Gym-style environment\n","        :param swap_channels: Swap image channels dimension from last to first [H, W, C] -> [C, H, W], defaults to False\n","        :type swap_channels: bool, optional\n","        :param max_steps: Maximum number of testing steps, defaults to None\n","        :type max_steps: int, optional\n","        :param loop: Number of testing loops/episodes to complete. The returned score is the mean over these tests. Defaults to 1\n","        :type loop: int, optional\n","        \"\"\"\n","        with torch.no_grad():\n","            rewards = []\n","            num_envs = env.num_envs if hasattr(env, \"num_envs\") else 1\n","            for i in range(loop):\n","                state, info = env.reset()\n","                scores = np.zeros(num_envs)\n","                completed_episode_scores = np.zeros(num_envs)\n","                finished = np.zeros(num_envs)\n","                step = 0\n","                while not np.all(finished):\n","                    if swap_channels:\n","                        state = np.moveaxis(state, [-1], [-3])\n","                    action_mask = info.get(\"action_mask\", None)\n","                    action = self.dqn_instance.get_action(state, epsilon=0, action_mask=action_mask)\n","                    state, reward, done, trunc, info = env.step(action[0])\n","                    step += 1\n","                    scores += np.array(reward)\n","                    for idx, (d, t) in enumerate(zip([done], [trunc])):\n","                        if (\n","                            d or t or (max_steps is not None and step == max_steps)\n","                        ) and not finished[idx]:\n","                            completed_episode_scores[idx] = scores[idx]\n","                            finished[idx] = 1\n","                rewards.append(np.mean(completed_episode_scores))\n","        mean_fit = np.mean(rewards)\n","        self.dqn_instance.fitness.append(mean_fit)\n","        return mean_fit\n","\n"],"metadata":{"id":"NWH4DdKhkCpJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wrapped_dqn = TupletoArrayDQN(trained_agent=agent)"],"metadata":{"id":"I4KIZGBMnKjQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Uses the Gym Monitor wrapper to evalaute the agent and record video\n","# only one video will be saved\n","# video of the final episode with the episode trigger\n","test_env = gym.wrappers.RecordVideo(\n","    test_env, \"./gym_monitor_output\", episode_trigger=lambda x: x == 0)\n","\n","wrapped_dqn.test(test_env, swap_channels=INIT_HP[\"CHANNELS_LAST\"], max_steps=INIT_HP['EVAL_STEPS'])\n","\n","test_env.close()"],"metadata":{"id":"HJPj-rXDtlHX","colab":{"base_uri":"https://localhost:8080/","height":138},"executionInfo":{"status":"ok","timestamp":1731191486341,"user_tz":480,"elapsed":303,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"}},"outputId":"6ed5b761-e6ab-4672-d4d6-fa4888b1626b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Moviepy - Building video /content/gym_monitor_output/rl-video-episode-0.mp4.\n","Moviepy - Writing video /content/gym_monitor_output/rl-video-episode-0.mp4\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n","t:   0%|          | 0/14 [00:00<?, ?it/s, now=None]\u001b[A\n","                                                   \u001b[A"]},{"output_type":"stream","name":"stdout","text":["Moviepy - Done !\n","Moviepy - video ready /content/gym_monitor_output/rl-video-episode-0.mp4\n"]}]}]}