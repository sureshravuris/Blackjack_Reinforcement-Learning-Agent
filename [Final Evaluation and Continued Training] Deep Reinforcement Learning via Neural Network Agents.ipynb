{"cells":[{"cell_type":"markdown","metadata":{"id":"KG071bmtVFFf"},"source":["# **Deep Q-Learning Network (DQN) Reinforcement Learning Agent for Blackjack**\n","In this notebook, we evaluate the final trained model using the ImprovedDQNAgent class within our training notebook. In our training, we used 500,000 epochs to train the agent using off-policy learning with a replaymemory."]},{"cell_type":"markdown","metadata":{"id":"AtUvlSMpqHc-"},"source":["## Imports and Installs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ancPCGb9eX9A"},"outputs":[],"source":["%%capture\n","# capture line is to hide the output\n","# Install required packages\n","!pip install gymnasium torch numpy matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLMCIhrzSIyj"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import random"]},{"cell_type":"markdown","metadata":{"id":"lum8T1EmWGpY"},"source":["## **DQN Agent with Replay Memory Implementation**\n","The Agent and the ReplayMemory class are instantiated here, and then the model weights are loaded. For additional improvement, it might be beneficial to save the agent and replaymemory as a class within a .py file for import."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_2UYM5EWDE2"},"outputs":[],"source":["import random\n","from collections import namedtuple, deque\n","\n","# Define a named tuple to store experiences\n","Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n","\n","class ReplayMemory:\n","    def __init__(self, capacity=10000):\n","        self.memory = deque(maxlen=capacity)\n","        self.capacity = capacity\n","\n","    def push(self, state, action, reward, next_state, done):\n","        \"\"\"Save an experience to memory\"\"\"\n","        experience = Experience(state, action, reward, next_state, done)\n","        self.memory.append(experience)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n","        if batch_size > len(self.memory):\n","            batch_size = len(self.memory)\n","        experiences = random.sample(self.memory, batch_size)\n","\n","        # Convert to separate arrays\n","        states = torch.FloatTensor([exp.state for exp in experiences])\n","        actions = torch.LongTensor([exp.action for exp in experiences])\n","        rewards = torch.FloatTensor([exp.reward for exp in experiences])\n","        next_states = torch.FloatTensor([exp.next_state for exp in experiences])\n","        dones = torch.FloatTensor([exp.done for exp in experiences])\n","\n","        return states, actions, rewards, next_states, dones\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","class ImprovedDQNAgent:\n","    def __init__(self, input_dim=3, learning_rate=5e-4, gamma=0.99, epsilon=1.0):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.fitness = [] # used to store a series of 'avg reward' during evaluation\n","\n","        # Larger network\n","        self.policy_net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 2)\n","        ).to(self.device)\n","\n","        self.target_net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 2)\n","        ).to(self.device)\n","\n","        self.target_net.load_state_dict(self.policy_net.state_dict())\n","\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n","        self.memory = ReplayMemory(capacity=20000)\n","\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.epsilon_min = 0.05\n","        self.epsilon_decay = 0.99997\n","        self.batch_size = 128\n","        self.target_update = 10\n","        self.episode_count = 0\n","\n","    def select_action(self, state):\n","        \"\"\"Select action using epsilon-greedy policy\"\"\"\n","        if random.random() < self.epsilon:\n","            return random.randint(0, 1)\n","\n","        with torch.no_grad():\n","            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n","            q_values = self.policy_net(state)\n","            return q_values.argmax().item()\n","\n","    def update_epsilon(self):\n","        \"\"\"Decay epsilon value\"\"\"\n","        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n","\n","    def store_transition(self, state, action, reward, next_state, done):\n","        \"\"\"Store transition in replay memory\"\"\"\n","        self.memory.push(state, action, reward, next_state, done)\n","\n","    def train_step(self):\n","        \"\"\"Perform one training step\"\"\"\n","        if len(self.memory) < self.batch_size:\n","            return\n","\n","        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n","        states = states.to(self.device)\n","        actions = actions.to(self.device)\n","        rewards = rewards.to(self.device)\n","        next_states = next_states.to(self.device)\n","        dones = dones.to(self.device)\n","\n","        # Double DQN implementation\n","        with torch.no_grad():\n","            next_actions = self.policy_net(next_states).argmax(1)\n","            next_q_values = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n","            target_q_values = rewards + (1 - dones.float()) * self.gamma * next_q_values\n","\n","        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","\n","        # Huber loss for better stability\n","        loss = nn.SmoothL1Loss()(current_q_values, target_q_values)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","    def update_target_network(self):\n","        \"\"\"Update target network parameters\"\"\"\n","        if self.episode_count % self.target_update == 0:\n","            self.target_net.load_state_dict(self.policy_net.state_dict())\n","        self.episode_count += 1"]},{"cell_type":"markdown","source":["The policy net state dictionary is the most important aspect of the training of the model. The target network is less useful as it learns without consideration of the states themselves, and is mostly used during training - whereas the policy net is  effective at inference."],"metadata":{"id":"rJbOR1UZ6Fx-"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191,"status":"ok","timestamp":1731619366532,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"},"user_tz":480},"id":"6ZGDNPckWZ0I","outputId":"1b2057ea-2c4b-4508-fac2-ecf8aa7c8663"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-7-6ea958eb1ffb>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  DQN_Agent.policy_net.load_state_dict(torch.load('/content/Blackjack_DQN_500000_episodes.pth')['policy_net_state_dict'])\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize the improved agent then loading the state dict from the .pth file\n","DQN_Agent = ImprovedDQNAgent()\n","DQN_Agent.policy_net.load_state_dict(torch.load('/content/Blackjack_DQN_500000_episodes.pth')['policy_net_state_dict'])"]},{"cell_type":"markdown","metadata":{"id":"XcpUXf7qa47U"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"4aUHVllYjxmR"},"source":["### Evaluation Metrics of the Trained Model on a New Environment\n","We create a new test environment using the RGB array with Gymnasium to visualize the agent's environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_EumUs-fk_8"},"outputs":[],"source":["test_env = gym.make(\"Blackjack-v1\", render_mode='rgb_array')"]},{"cell_type":"markdown","source":["Our evaluation function removes some components that were used in the training function and mostly captures the rewards - such as whether the agent was able to play optimally in terms of the number of wins, draws, and losses."],"metadata":{"id":"r-5qRlHe6mG8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1731619379327,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"},"user_tz":480},"id":"p4Z1zz0vikvC","outputId":"4b6fe67f-d383-484f-cc3c-bc444b29dce9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Evaluating trained agent...\n","\n","Detailed Evaluation Results:\n","Number of Episodes: 200\n","Win Rate: 38.0%  (76/200)\n","Draw Rate: 8.5%  (17/200)\n","Loss Rate: 53.5%  (107/200)\n","Average Reward: -0.155\n","Average Final Player Sum: 19.7\n"]}],"source":["# Evaluate the trained agent\n","def evaluate_agent_detailed(test_env, trained_agent, n_episodes=200):\n","    wins = 0\n","    draws = 0\n","    losses = 0\n","    total_rewards = []\n","    player_sums = []\n","    dealer_sums = []\n","\n","    for episode in range(n_episodes):\n","        state, _ = test_env.reset()\n","        done = False\n","        episode_reward = 0\n","\n","        while not done:\n","            # Use greedy policy (no exploration)\n","            with torch.no_grad():\n","                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n","                action = trained_agent.policy_net(state_tensor).argmax().item()\n","\n","            state, reward, done, truncated, _ = test_env.step(action)\n","            episode_reward += reward\n","\n","        total_rewards.append(episode_reward)\n","        if reward > 0:\n","            wins += 1\n","        elif reward == 0:\n","            draws += 1\n","        else:\n","            losses += 1\n","\n","        player_sums.append(state[0])  # Final player sum\n","\n","    print(\"\\nDetailed Evaluation Results:\")\n","    print(f\"Number of Episodes: {n_episodes}\")\n","    print(f\"Win Rate: {wins/n_episodes*100:.1f}%  ({wins}/{n_episodes})\")\n","    print(f\"Draw Rate: {draws/n_episodes*100:.1f}%  ({draws}/{n_episodes})\")\n","    print(f\"Loss Rate: {losses/n_episodes*100:.1f}%  ({losses}/{n_episodes})\")\n","    print(f\"Average Reward: {np.mean(total_rewards):.3f}\")\n","    print(f\"Average Final Player Sum: {np.mean(player_sums):.1f}\")\n","\n","    return total_rewards, player_sums\n","\n","# Evaluate the trained agent\n","print(\"\\nEvaluating trained agent...\")\n","eval_rewards, player_sums = evaluate_agent_detailed(test_env, DQN_Agent, n_episodes=200)"]},{"cell_type":"markdown","metadata":{"id":"y2QJAaa2fvcc"},"source":["### Visualizing the Agent's Learning with Gymnasium's RGB Environment\n","This visualization is less useful for evaluation and is mostly useful for interpretting the gameplay. The evaluation output is informative and tells us the statistics of how the agent fares within the evaluation environment. We found that our agent usually performs within the 40-50% win range. This is quite normal as blackjack is at most a game of luck, and in general even the optimal stategy has a .05% disadvantage compared to the dealer/the antagonist of the game."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1242,"status":"ok","timestamp":1731619383684,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"},"user_tz":480},"id":"AwyJyBPkfl39","outputId":"8cb3db71-8747-42e5-dc51-b13876a9b8cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Detailed Evaluation Results:\n","Number of Episodes: 200\n","Win Rate: 39.0%  (78/200)\n","Draw Rate: 11.0%  (22/200)\n","Loss Rate: 50.0%  (100/200)\n","Average Reward: -0.110\n","Average Final Player Sum: 19.6\n"]}],"source":["# Uses the Gym Monitor wrapper to evalaute the agent and record video\n","# only one video will be saved\n","# video of the final episode with the episode trigger\n","test_env = gym.wrappers.RecordVideo(\n","    test_env, \"./gym_monitor_output\", episode_trigger=lambda x: x == 0)\n","\n","evaluate_agent_detailed(test_env, DQN_Agent)\n","\n","test_env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlJGfS08nMwd"},"outputs":[],"source":["# play a video using a path to the video\n","from IPython.display import Video\n","from base64 import b64encode\n","\n","def show_video(video_path):\n","    video_file = Video(video_path, embed=True)\n","    display(video_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1731619390464,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"},"user_tz":480},"id":"oMwgylKlnaDn","outputId":"653f2c9e-bc96-4aaf-fc84-d1ed29f1ef27"},"outputs":[{"data":{"text/html":["<video controls  >\n"," <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n"," Your browser does not support the video tag.\n"," </video>"],"text/plain":["<IPython.core.display.Video object>"]},"metadata":{},"output_type":"display_data"}],"source":["# visualizing the rl-video-episode-0.mp4 in the gym_monitor_output\n","show_video(\"./gym_monitor_output/rl-video-episode-0.mp4\")"]},{"cell_type":"markdown","metadata":{"id":"lpSdmCd8jOY_"},"source":["## Strategy Analysis and Interactive Human Feedback Loop with A.I Recommendation\n","Finally, we implemented a stategy analysis to understand the kinds of decisions that were learned by the agent within the Q-Learning policy matrix. We also implemented a human feedback loop with the agent's action recommendations. The training loop retrains the model after each game and helps to train the model using the human's actions."]},{"cell_type":"markdown","metadata":{"id":"BcTSx1uqjaNj"},"source":["### Strategy Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1731619395508,"user":{"displayName":"Suresh Ravuri","userId":"05000175820317869228"},"user_tz":480},"id":"6m6LhAQLfZCX","outputId":"eaed8129-6ec9-434a-f136-95f325bb033d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Strategy Analysis:\n","Player Sum | Dealer Card | Action\n","-----------------------------------\n","    16     |     10      |  Hit  \n","    12     |      6      | Stand \n","    18     |      9      | Stand \n","    11     |     10      |  Hit  \n","    15     |      7      |  Hit  \n"]}],"source":["def analyze_strategy(agent):\n","    # Common situations in blackjack\n","    test_states = [\n","        (16, 10, 0),  # Hard 16 vs dealer 10\n","        (12, 6, 0),   # Hard 12 vs dealer 6\n","        (18, 9, 0),   # Hard 18 vs dealer 9\n","        (11, 10, 0),  # Hard 11 vs dealer 10\n","        (15, 7, 0),   # Hard 15 vs dealer 7\n","    ]\n","\n","    print(\"\\nStrategy Analysis:\")\n","    print(\"Player Sum | Dealer Card | Action\")\n","    print(\"-\" * 35)\n","\n","    for player_sum, dealer_card, usable_ace in test_states:\n","        state = np.array([player_sum, dealer_card, usable_ace])\n","        with torch.no_grad():\n","            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n","            action = agent.policy_net(state_tensor).argmax().item()\n","            action_name = \"Hit\" if action == 1 else \"Stand\"\n","            print(f\"{player_sum:^10} | {dealer_card:^11} | {action_name:^6}\")\n","\n","# Run strategy analysis\n","analyze_strategy(DQN_Agent)"]},{"cell_type":"markdown","metadata":{"id":"kVFaJR1rjcSx"},"source":["### Interactive Human Feedback Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"U3dEFiVDf_W9","outputId":"2b9c5c2c-2d44-4b8a-a401-865594d8c6b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Starting interactive learning blackjack game...\n","\n","Welcome to Interactive Learning Blackjack!\n","The AI will learn from your games.\n","\n","Dealer shows: [K, ?]\n","Your cards: [8, 6]\n","Your total: 14\n","\n","AI Recommends: Hit (Confidence: 0.56)\n","\n","Dealer shows: [K, ?]\n","Your cards: [8, 6, Q]\n","Your total: 24\n","Bust! You lose.\n","\n","Current Stats:\n","Games Played: 1\n","Win Rate: 0.0%\n","\n","Dealer shows: [4, ?]\n","Your cards: [7, J]\n","Your total: 17\n","\n","AI Recommends: Stand (Confidence: 0.62)\n","Dealer hits: [4, A, 3] (Total: 18)\n","\n","Final hands:\n","Dealer: [4, A, 3] (Total: 18)\n","Player: [7, J] (Total: 17)\n","Dealer wins!\n","\n","Current Stats:\n","Games Played: 2\n","Win Rate: 0.0%\n","\n","Dealer shows: [4, ?]\n","Your cards: [J, 10]\n","Your total: 20\n","\n","AI Recommends: Stand (Confidence: 0.86)\n","Dealer hits: [4, 10, A] (Total: 15)\n","Dealer hits: [4, 10, A, 6] (Total: 21)\n","\n","Final hands:\n","Dealer: [4, 10, A, 6] (Total: 21)\n","Player: [J, 10] (Total: 20)\n","Dealer wins!\n","\n","Current Stats:\n","Games Played: 3\n","Win Rate: 0.0%\n","\n","Dealer shows: [Q, ?]\n","Your cards: [K, 8]\n","Your total: 18\n","\n","AI Recommends: Stand (Confidence: 0.55)\n","\n","Dealer shows: [Q, ?]\n","Your cards: [K, 8, J]\n","Your total: 28\n","Bust! You lose.\n","\n","Current Stats:\n","Games Played: 4\n","Win Rate: 0.0%\n","\n","Dealer shows: [Q, ?]\n","Your cards: [7, 10]\n","Your total: 17\n","\n","AI Recommends: Stand (Confidence: 0.50)\n","\n","Final hands:\n","Dealer: [Q, 9] (Total: 19)\n","Player: [7, 10] (Total: 17)\n","Dealer wins!\n","\n","Current Stats:\n","Games Played: 5\n","Win Rate: 0.0%\n","\n","Dealer shows: [7, ?]\n","Your cards: [K, 2]\n","Your total: 12\n","\n","AI Recommends: Hit (Confidence: 0.62)\n","\n","Dealer shows: [7, ?]\n","Your cards: [K, 2, J]\n","Your total: 22\n","Bust! You lose.\n","\n","Current Stats:\n","Games Played: 6\n","Win Rate: 0.0%\n","\n","Dealer shows: [J, ?]\n","Your cards: [7, 5]\n","Your total: 12\n","\n","AI Recommends: Hit (Confidence: 0.60)\n","\n","Dealer shows: [J, ?]\n","Your cards: [7, 5, 3]\n","Your total: 15\n","\n","AI Recommends: Hit (Confidence: 0.55)\n","\n","Dealer shows: [J, ?]\n","Your cards: [7, 5, 3, Q]\n","Your total: 25\n","Bust! You lose.\n","Dealer hits: [J, 3, 7] (Total: 20)\n","\n","Current Stats:\n","Games Played: 7\n","Win Rate: 0.0%\n","\n","Dealer shows: [9, ?]\n","Your cards: [2, Q]\n","Your total: 12\n","\n","AI Recommends: Hit (Confidence: 0.60)\n","\n","Dealer shows: [9, ?]\n","Your cards: [2, Q, 6]\n","Your total: 18\n","\n","AI Recommends: Stand (Confidence: 0.60)\n","\n","Final hands:\n","Dealer: [9, K] (Total: 19)\n","Player: [2, Q, 6] (Total: 18)\n","Dealer wins!\n","\n","Current Stats:\n","Games Played: 8\n","Win Rate: 0.0%\n","\n","Dealer shows: [10, ?]\n","Your cards: [4, Q]\n","Your total: 14\n","\n","AI Recommends: Hit (Confidence: 0.56)\n","\n","Dealer shows: [10, ?]\n","Your cards: [4, Q, 2]\n","Your total: 16\n","\n","AI Recommends: Hit (Confidence: 0.52)\n","\n","Dealer shows: [10, ?]\n","Your cards: [4, Q, 2, K]\n","Your total: 26\n","Bust! You lose.\n","\n","Current Stats:\n","Games Played: 9\n","Win Rate: 0.0%\n","\n","Dealer shows: [10, ?]\n","Your cards: [10, K]\n","Your total: 20\n","\n","AI Recommends: Stand (Confidence: 0.81)\n","Dealer hits: [10, 4, 10] (Total: 24)\n","\n","Final hands:\n","Dealer: [10, 4, 10] (Total: 24)\n","Player: [10, K] (Total: 20)\n","Dealer busts! You win!\n","\n","Current Stats:\n","Games Played: 10\n","Win Rate: 10.0%\n","\n","Dealer shows: [8, ?]\n","Your cards: [A, 5]\n","Your total: 16\n","\n","AI Recommends: Hit (Confidence: 0.71)\n","\n","Dealer shows: [8, ?]\n","Your cards: [A, 5, 3]\n","Your total: 19\n","\n","AI Recommends: Stand (Confidence: 0.63)\n","Dealer hits: [8, 4, 4] (Total: 16)\n","Dealer hits: [8, 4, 4, K] (Total: 26)\n","\n","Final hands:\n","Dealer: [8, 4, 4, K] (Total: 26)\n","Player: [A, 5, 3] (Total: 19)\n","Dealer busts! You win!\n","\n","Current Stats:\n","Games Played: 11\n","Win Rate: 18.2%\n","\n","Dealer shows: [3, ?]\n","Your cards: [7, K]\n","Your total: 17\n","\n","AI Recommends: Stand (Confidence: 0.61)\n","Dealer hits: [3, 7, Q] (Total: 20)\n","\n","Final hands:\n","Dealer: [3, 7, Q] (Total: 20)\n","Player: [7, K] (Total: 17)\n","Dealer wins!\n","\n","Current Stats:\n","Games Played: 12\n","Win Rate: 16.7%\n","\n","Dealer shows: [J, ?]\n","Your cards: [7, A]\n","Your total: 18\n","\n","AI Recommends: Hit (Confidence: 0.58)\n","\n","Dealer shows: [J, ?]\n","Your cards: [7, A, 10]\n","Your total: 18\n","\n","AI Recommends: Hit (Confidence: 0.58)\n","\n","Dealer shows: [J, ?]\n","Your cards: [7, A, 10, 3]\n","Your total: 21\n","\n","AI Recommends: Stand (Confidence: 0.69)\n","\n","Final hands:\n","Dealer: [J, 9] (Total: 19)\n","Player: [7, A, 10, 3] (Total: 21)\n","You win!\n","\n","Current Stats:\n","Games Played: 13\n","Win Rate: 23.1%\n","\n","Dealer shows: [3, ?]\n","Your cards: [Q, 6]\n","Your total: 16\n","\n","AI Recommends: Stand (Confidence: 0.52)\n","Dealer hits: [3, A, 7] (Total: 21)\n","\n","Final hands:\n","Dealer: [3, A, 7] (Total: 21)\n","Player: [Q, 6] (Total: 16)\n","Dealer wins!\n","\n","Current Stats:\n","Games Played: 14\n","Win Rate: 21.4%\n","\n","Dealer shows: [K, ?]\n","Your cards: [6, 4]\n","Your total: 10\n","\n","AI Recommends: Hit (Confidence: 0.63)\n","\n","Dealer shows: [K, ?]\n","Your cards: [6, 4, Q]\n","Your total: 20\n","\n","AI Recommends: Stand (Confidence: 0.81)\n","Dealer hits: [K, 2, 8] (Total: 20)\n","\n","Final hands:\n","Dealer: [K, 2, 8] (Total: 20)\n","Player: [6, 4, Q] (Total: 20)\n","Push (tie)!\n","\n","Current Stats:\n","Games Played: 15\n","Win Rate: 20.0%\n"]}],"source":["def card_name(card):\n","    \"\"\"Convert card number to readable format\"\"\"\n","    if card == 1:\n","        return 'A'\n","    elif card == 11:\n","        return 'J'\n","    elif card == 12:\n","        return 'Q'\n","    elif card == 13:\n","        return 'K'\n","    else:\n","        return str(card)\n","\n","def print_cards(cards, hidden=False):\n","    \"\"\"Display cards in readable format\"\"\"\n","    if hidden:\n","        return f\"[{card_name(cards[0])}, ?]\"\n","    return f\"[{', '.join(card_name(c) for c in cards)}]\"\n","\n","def get_card_value(card):\n","    if card == 1:  # Ace\n","        return 11\n","    return min(card, 10)\n","\n","def calculate_hand_value(cards):\n","    value = sum(get_card_value(card) for card in cards)\n","    num_aces = cards.count(1)\n","\n","    # Adjust for aces\n","    while value > 21 and num_aces:\n","        value -= 10\n","        num_aces -= 1\n","\n","    return value\n","\n","def play_interactive_blackjack_with_learning(agent):\n","    print(\"\\nWelcome to Interactive Learning Blackjack!\")\n","    print(\"The AI will learn from your games.\")\n","\n","    game_memory = []  # Store game experiences\n","    stats = {'games': 0, 'wins': 0, 'losses': 0, 'draws': 0}\n","\n","    while True:\n","        player_cards = []\n","        dealer_cards = []\n","        deck = list(range(1, 14)) * 4\n","        random.shuffle(deck)\n","\n","        # Initial deal\n","        player_cards.extend([deck.pop(), deck.pop()])\n","        dealer_cards.extend([deck.pop(), deck.pop()])\n","\n","        game_states = []  # Store states for this game\n","\n","        while True:\n","            print(\"\\nDealer shows:\", print_cards(dealer_cards, hidden=True))\n","            print(\"Your cards:\", print_cards(player_cards))\n","            player_value = calculate_hand_value(player_cards)\n","            print(f\"Your total: {player_value}\")\n","\n","            if player_value > 21:\n","                print(\"Bust! You lose.\")\n","                stats['losses'] += 1\n","                break\n","\n","            # Current state\n","            current_state = np.array([player_value, get_card_value(dealer_cards[0]), 1 in player_cards])\n","\n","            # Get AI recommendation\n","            with torch.no_grad():\n","                state_tensor = torch.FloatTensor(current_state).unsqueeze(0)\n","                q_values = agent.policy_net(state_tensor)\n","                ai_action = q_values.argmax().item()\n","                confidence = torch.softmax(q_values, dim=1)[0]\n","                ai_recommendation = \"Hit\" if ai_action == 1 else \"Stand\"\n","                print(f\"\\nAI Recommends: {ai_recommendation} (Confidence: {confidence[ai_action]:.2f})\")\n","\n","            action = input(\"\\nYour action (H/S): \").upper()\n","            while action not in ['H', 'S']:\n","                action = input(\"Invalid input. Please enter H or S: \").upper()\n","\n","            # Store state and action\n","            game_states.append((\n","                current_state,\n","                1 if action == 'H' else 0,\n","                player_value\n","            ))\n","\n","            if action == 'H':\n","                player_cards.append(deck.pop())\n","            else:\n","                break\n","\n","        # Game ended, calculate final reward\n","        final_player_value = calculate_hand_value(player_cards)\n","        dealer_value = play_dealer_hand(dealer_cards, deck)\n","\n","        if final_player_value <= 21:\n","            print(\"\\nFinal hands:\")\n","            print(f\"Dealer: {print_cards(dealer_cards)} (Total: {dealer_value})\")\n","            print(f\"Player: {print_cards(player_cards)} (Total: {final_player_value})\")\n","\n","            if dealer_value > 21:\n","                print(\"Dealer busts! You win!\")\n","                reward = 1.0\n","                stats['wins'] += 1\n","            elif dealer_value > final_player_value:\n","                print(\"Dealer wins!\")\n","                reward = -1.0\n","                stats['losses'] += 1\n","            elif dealer_value < final_player_value:\n","                print(\"You win!\")\n","                reward = 1.0\n","                stats['wins'] += 1\n","            else:\n","                print(\"Push (tie)!\")\n","                reward = 0.0\n","                stats['draws'] += 1\n","        else:\n","            reward = -1.0\n","\n","        # Store experiences for learning\n","        for state, action, value in game_states:\n","            agent.memory.push(state, action, reward, state, True)\n","            # Train on a batch\n","            if len(agent.memory) >= agent.batch_size:\n","                loss = agent.train_step()\n","                if loss:\n","                    print(f\"Training loss: {loss:.4f}\")\n","\n","        stats['games'] += 1\n","        print(\"\\nCurrent Stats:\")\n","        print(f\"Games Played: {stats['games']}\")\n","        print(f\"Win Rate: {stats['wins']/stats['games']*100:.1f}%\")\n","\n","        play_again = input(\"\\nPlay again? (Y/N): \").upper()\n","        if play_again != 'Y':\n","            break\n","\n","    print(\"\\nFinal Stats:\")\n","    print(f\"Games Played: {stats['games']}\")\n","    print(f\"Wins: {stats['wins']}\")\n","    print(f\"Losses: {stats['losses']}\")\n","    print(f\"Draws: {stats['draws']}\")\n","    print(f\"Win Rate: {stats['wins']/stats['games']*100:.1f}%\")\n","\n","    # Save the improved model\n","    torch.save({\n","        'policy_net_state_dict': agent.policy_net.state_dict(),\n","        'optimizer_state_dict': agent.optimizer.state_dict(),\n","        'epsilon': agent.epsilon,\n","    }, 'blackjack_dqn_improved.pth')\n","    print(\"\\nImproved model saved!\")\n","\n","# Helper function for dealer's turn\n","def play_dealer_hand(dealer_cards, deck):\n","    dealer_value = calculate_hand_value(dealer_cards)\n","    while dealer_value < 17:\n","        dealer_cards.append(deck.pop())\n","        dealer_value = calculate_hand_value(dealer_cards)\n","        print(f\"Dealer hits: {print_cards(dealer_cards)} (Total: {dealer_value})\")\n","    return dealer_value\n","\n","# Start the learning interactive game\n","print(\"\\nStarting interactive learning blackjack game...\")\n","play_interactive_blackjack_with_learning(DQN_Agent)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}